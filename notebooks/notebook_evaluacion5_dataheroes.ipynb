{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9e61e8",
   "metadata": {},
   "source": [
    "# Evaluación 5\n",
    "\n",
    "```yaml\n",
    "Equipo: Data Heroes\n",
    "Integrantes:\n",
    "    - Paul E. Arizpe Colorado\n",
    "    - Tania Gúzman Aguirre\n",
    "    - M. Fernanda Martínez Campos\n",
    "Grupo: 801\n",
    "Carrera: LCDN\n",
    "Sede: Coyoacán\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ccd8b",
   "metadata": {},
   "source": [
    "### Carga de librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce861285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to E:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to E:\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to E:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import hmm\n",
    "from nltk.text import TextCollection\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime as dt\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer, one_hot\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Descargamos los recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargamos las stopwords en español\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "# Para realizar el stemming\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40425bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n",
      "2.19.0\n",
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__version__)\n",
    "print(tf.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69de745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de documento\n",
    "def load_doc(pathfile):\n",
    "    with open(pathfile, 'rt', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3c78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte 30, día 30\n",
      "El taller Popocatépetl Motors ( propiedad de \n",
      "@LuisMendozaBJ\n",
      " ) usa el carril del RTP como estacionamiento y extensión de su patio de maniobras, bloquea banquetas y pasos peatonales. Debido a esto es el causante de varios accidentes viales. \n",
      "@UCS_GCDMX\n",
      " \n",
      "@SSC_CDMX\n"
     ]
    }
   ],
   "source": [
    "# Test Load Document\n",
    "doc = load_doc('../../tweets/quejas/1.txt')\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda2235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de texto\n",
    "def clean_text(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Eliminar signos de puntuación y convertir a minúsculas\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # Eliminar palabras de un solo carácter\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357dbeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reporte', 'día', 'taller', 'popocatépetl', 'motors', 'propiedad', 'luismendozabj', 'usa', 'carril', 'rtp']\n"
     ]
    }
   ],
   "source": [
    "# test clean_text\n",
    "tokens = clean_text(doc)\n",
    "print(tokens[:10])  # Muestra las primeras 10 palabras tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ac14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def doc_to_line(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_text(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36c1ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reportertp\n"
     ]
    }
   ],
   "source": [
    "# Test doc_to_line\n",
    "filename = '../../tweets/quejas/1.txt'\n",
    "vocab = Counter({'rtp': 1, 'queja': 2, 'reclamo': 3, 'servicio': 4, 'reporte': 5})\n",
    "doc2line = doc_to_line(filename, vocab)\n",
    "print(doc2line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1530436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('rtp', 1): 1, ('queja', 1): 1})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "576321ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar los documentos de quejas y sugerencias\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    \n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        pathfile = rf'{directory}/{filename}'\n",
    "        texto = doc_to_line(pathfile, vocab)\n",
    "        print(f'Archivo cargado: {filename}')\n",
    "        print(texto)\n",
    "        lines.append(texto)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bfa01a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7362933e9240ed9ed9dfdb51e619d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: 1.txt\n",
      "reportertp\n",
      "Archivo cargado: 10.txt\n",
      "rtpservicioservicio\n",
      "Archivo cargado: 11.txt\n",
      "servicio\n",
      "Archivo cargado: 12.txt\n",
      "serviciortp\n",
      "Archivo cargado: 14.txt\n",
      "\n",
      "Archivo cargado: 16.txt\n",
      "servicio\n",
      "Archivo cargado: 18.txt\n",
      "\n",
      "Archivo cargado: 31.txt\n",
      "serviciortp\n",
      "Archivo cargado: 32.txt\n",
      "\n",
      "Archivo cargado: 33.txt\n",
      "rtp\n",
      "Archivo cargado: 34.txt\n",
      "rtpservicio\n",
      "Archivo cargado: 35.txt\n",
      "quejartp\n",
      "Archivo cargado: 36.txt\n",
      "rtp\n",
      "Archivo cargado: 37.txt\n",
      "rtp\n",
      "Archivo cargado: 38.txt\n",
      "\n",
      "Archivo cargado: 39.txt\n",
      "serviciortp\n",
      "Archivo cargado: 40.txt\n",
      "\n",
      "Archivo cargado: 5.txt\n",
      "rtp\n",
      "Archivo cargado: 7.txt\n",
      "servicio\n",
      "Archivo cargado: 8.txt\n",
      "rtp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['reportertp',\n",
       " 'rtpservicioservicio',\n",
       " 'servicio',\n",
       " 'serviciortp',\n",
       " '',\n",
       " 'servicio',\n",
       " '',\n",
       " 'serviciortp',\n",
       " '',\n",
       " 'rtp',\n",
       " 'rtpservicio',\n",
       " 'quejartp',\n",
       " 'rtp',\n",
       " 'rtp',\n",
       " '',\n",
       " 'serviciortp',\n",
       " '',\n",
       " 'rtp',\n",
       " 'servicio',\n",
       " 'rtp']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test process_docs\n",
    "directory = '../../tweets/quejas'\n",
    "is_train = True\n",
    "lines = process_docs(directory, vocab, is_train)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d56697b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filname):\n",
    "    data = '\\n'.join(lines)\n",
    "    with open(filname, 'w') as file:\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f26db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Actualizar el vocabulario con las palabras procesadas de cada documento\n",
    "def update_vocab(vocab, tokens):\n",
    "    vocab.update(tokens)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7dea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Procesar los documentos de las quejas y sugerencias y generar un vocabulario.\n",
    "def process_docs_and_update_vocab(directory, vocab):\n",
    "    print(f'Procesando documentos en el directorio: {directory}')\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('.txt'):\n",
    "            pathfile = f'{directory}/{filename}'\n",
    "            texto = load_doc(pathfile)\n",
    "            tokens = clean_text(texto)\n",
    "            vocab = update_vocab(vocab, tokens)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0175dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Filtrar las palabras que aparecen con una frecuencia mayor o igual al umbral (5 en este caso) y guardarlas en un archivo.\n",
    "def save_vocab(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    if os.path.exists(filename):\n",
    "        # Si existe el archivo, agregamos el nuevo vocabulario al final\n",
    "        with open(filename, 'a+', encoding='utf-8') as file:\n",
    "            file.write('\\n')\n",
    "            file.write(data)\n",
    "    else:\n",
    "        # No existe el archivo, lo creamos\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(data)\n",
    "    print(f'Vocabulario guardado en: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d7d9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(dir1, dir2, vocab, is_train):\n",
    "    negative = process_docs(dir1, vocab, is_train)\n",
    "    positive = process_docs(dir2, vocab, is_train)\n",
    "\n",
    "    docs = negative + positive #Lista que contiene tanto las quejas como las sugerencias procesadas\n",
    "    \n",
    "    labels = [0 for _ in range(len(negative))] + [1 for _ in range(len(positive))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123547d",
   "metadata": {},
   "source": [
    "### Preparación de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4467f86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830109fb513f4e078b1d99621e8f830d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando documentos en el directorio: ../../tweets/quejas/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fbcea9a59a42b5ae49e350fff900f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando documentos en el directorio: ../../tweets/sugerencias/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492c5bb53a2443089f264a6bf471acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario guardado en: vocab.txt\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "umbral = 5\n",
    "tokens = []\n",
    "directories = ['../../tweets/quejas/', '../../tweets/sugerencias/']\n",
    "for directory in tqdm(directories):\n",
    "    vocab = process_docs_and_update_vocab(directory, vocab)\n",
    "    tokens += [k for k, c in vocab.items() if c >= umbral]\n",
    "\n",
    "# Guardamos el vocabulario en un archivo\n",
    "unique_vocab = set(tokens)\n",
    "save_vocab(unique_vocab, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "741fc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'si', 'ciudad', 'ruta', 'hoy', 'servicio', 'día', 'rtpciudaddemex', 'rtp', 'rutas', 'metro', 'mirtp', 'va', 'unidades', 'personas', 'santa'}\n"
     ]
    }
   ],
   "source": [
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06eb7526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c430bd3abc204dd1aa4a41833edc0838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: 1.txt\n",
      "díartp\n",
      "Archivo cargado: 10.txt\n",
      "rtpciudadserviciometroserviciortpciudaddemex\n",
      "Archivo cargado: 11.txt\n",
      "metroserviciounidadesrtpciudaddemexpersonas\n",
      "Archivo cargado: 12.txt\n",
      "serviciortpciudaddemexrutasrtpsantametro\n",
      "Archivo cargado: 14.txt\n",
      "\n",
      "Archivo cargado: 16.txt\n",
      "servicio\n",
      "Archivo cargado: 18.txt\n",
      "\n",
      "Archivo cargado: 31.txt\n",
      "serviciorutasantapersonasrtp\n",
      "Archivo cargado: 32.txt\n",
      "rutasantava\n",
      "Archivo cargado: 33.txt\n",
      "rtp\n",
      "Archivo cargado: 34.txt\n",
      "rtpciudadunidadesrutaservicio\n",
      "Archivo cargado: 35.txt\n",
      "rtpdíahoyrutasanta\n",
      "Archivo cargado: 36.txt\n",
      "rtp\n",
      "Archivo cargado: 37.txt\n",
      "rtp\n",
      "Archivo cargado: 38.txt\n",
      "\n",
      "Archivo cargado: 39.txt\n",
      "serviciortpmetrounidadesunidades\n",
      "Archivo cargado: 40.txt\n",
      "\n",
      "Archivo cargado: 5.txt\n",
      "rtpmetro\n",
      "Archivo cargado: 7.txt\n",
      "serviciorutartpciudaddemex\n",
      "Archivo cargado: 8.txt\n",
      "rtpdía\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8ed5033e984d00a710b09e318a64c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: 13.txt\n",
      "mirtppersonas\n",
      "Archivo cargado: 15.txt\n",
      "mirtpdíadía\n",
      "Archivo cargado: 17.txt\n",
      "vahoy\n",
      "Archivo cargado: 19.txt\n",
      "mirtprutasantapersonasservicio\n",
      "Archivo cargado: 2.txt\n",
      "mirtp\n",
      "Archivo cargado: 20.txt\n",
      "rutasvartprutasunidadesrutasi\n",
      "Archivo cargado: 21.txt\n",
      "rutasvartprutasunidadesrutasi\n",
      "Archivo cargado: 22.txt\n",
      "\n",
      "Archivo cargado: 23.txt\n",
      "unidadessivartp\n",
      "Archivo cargado: 24.txt\n",
      "rtpciudaddemex\n",
      "Archivo cargado: 25.txt\n",
      "unidadessi\n",
      "Archivo cargado: 26.txt\n",
      "\n",
      "Archivo cargado: 27.txt\n",
      "serviciohoyrtpciudadsiservicio\n",
      "Archivo cargado: 28.txt\n",
      "unidadesrtpciudad\n",
      "Archivo cargado: 29.txt\n",
      "hoy\n",
      "Archivo cargado: 3.txt\n",
      "rutartpciudaddemexdíaciudadrtpmetrociudad\n",
      "Archivo cargado: 30.txt\n",
      "mirtpservicio\n",
      "Archivo cargado: 4.txt\n",
      "mirtprutasanta\n",
      "Archivo cargado: 6.txt\n",
      "rtp\n",
      "Archivo cargado: 9.txt\n",
      "rutartphoyrtpciudaddemexsantapersonas\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bca402fec04a62bed45acc82f67ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: 1.txt\n",
      "díartp\n",
      "Archivo cargado: 10.txt\n",
      "rtpciudadserviciometroserviciortpciudaddemex\n",
      "Archivo cargado: 11.txt\n",
      "metroserviciounidadesrtpciudaddemexpersonas\n",
      "Archivo cargado: 12.txt\n",
      "serviciortpciudaddemexrutasrtpsantametro\n",
      "Archivo cargado: 14.txt\n",
      "\n",
      "Archivo cargado: 16.txt\n",
      "servicio\n",
      "Archivo cargado: 18.txt\n",
      "\n",
      "Archivo cargado: 31.txt\n",
      "serviciorutasantapersonasrtp\n",
      "Archivo cargado: 32.txt\n",
      "rutasantava\n",
      "Archivo cargado: 33.txt\n",
      "rtp\n",
      "Archivo cargado: 34.txt\n",
      "rtpciudadunidadesrutaservicio\n",
      "Archivo cargado: 35.txt\n",
      "rtpdíahoyrutasanta\n",
      "Archivo cargado: 36.txt\n",
      "rtp\n",
      "Archivo cargado: 37.txt\n",
      "rtp\n",
      "Archivo cargado: 38.txt\n",
      "\n",
      "Archivo cargado: 39.txt\n",
      "serviciortpmetrounidadesunidades\n",
      "Archivo cargado: 40.txt\n",
      "\n",
      "Archivo cargado: 5.txt\n",
      "rtpmetro\n",
      "Archivo cargado: 7.txt\n",
      "serviciorutartpciudaddemex\n",
      "Archivo cargado: 8.txt\n",
      "rtpdía\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeaa1f849124b5f9c06c25b64fb9257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: 13.txt\n",
      "mirtppersonas\n",
      "Archivo cargado: 15.txt\n",
      "mirtpdíadía\n",
      "Archivo cargado: 17.txt\n",
      "vahoy\n",
      "Archivo cargado: 19.txt\n",
      "mirtprutasantapersonasservicio\n",
      "Archivo cargado: 2.txt\n",
      "mirtp\n",
      "Archivo cargado: 20.txt\n",
      "rutasvartprutasunidadesrutasi\n",
      "Archivo cargado: 21.txt\n",
      "rutasvartprutasunidadesrutasi\n",
      "Archivo cargado: 22.txt\n",
      "\n",
      "Archivo cargado: 23.txt\n",
      "unidadessivartp\n",
      "Archivo cargado: 24.txt\n",
      "rtpciudaddemex\n",
      "Archivo cargado: 25.txt\n",
      "unidadessi\n",
      "Archivo cargado: 26.txt\n",
      "\n",
      "Archivo cargado: 27.txt\n",
      "serviciohoyrtpciudadsiservicio\n",
      "Archivo cargado: 28.txt\n",
      "unidadesrtpciudad\n",
      "Archivo cargado: 29.txt\n",
      "hoy\n",
      "Archivo cargado: 3.txt\n",
      "rutartpciudaddemexdíaciudadrtpmetrociudad\n",
      "Archivo cargado: 30.txt\n",
      "mirtpservicio\n",
      "Archivo cargado: 4.txt\n",
      "mirtprutasanta\n",
      "Archivo cargado: 6.txt\n",
      "rtp\n",
      "Archivo cargado: 9.txt\n",
      "rutartphoyrtpciudaddemexsantapersonas\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos de entrenamiento y prueba\n",
    "dir1 = r'D:\\Workspace\\ws_python\\computo_cognitivo_labs\\tweets\\quejas'\n",
    "dir2 = r'D:\\Workspace\\ws_python\\computo_cognitivo_labs\\tweets\\sugerencias'\n",
    "train_docs, y_train = load_clean_dataset(dir1, dir2, vocab, is_train=True)\n",
    "test_docs, y_test = load_clean_dataset(dir1, dir2, vocab, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f04ebe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el tokenizador y entrenarlo con los datos de entrenamiento\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "66723a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['díartp',\n",
       "  'rtpciudadserviciometroserviciortpciudaddemex',\n",
       "  'metroserviciounidadesrtpciudaddemexpersonas',\n",
       "  'serviciortpciudaddemexrutasrtpsantametro',\n",
       "  '',\n",
       "  'servicio',\n",
       "  '',\n",
       "  'serviciorutasantapersonasrtp',\n",
       "  'rutasantava',\n",
       "  'rtp',\n",
       "  'rtpciudadunidadesrutaservicio',\n",
       "  'rtpdíahoyrutasanta',\n",
       "  'rtp',\n",
       "  'rtp',\n",
       "  '',\n",
       "  'serviciortpmetrounidadesunidades',\n",
       "  '',\n",
       "  'rtpmetro',\n",
       "  'serviciorutartpciudaddemex',\n",
       "  'rtpdía',\n",
       "  'mirtppersonas',\n",
       "  'mirtpdíadía',\n",
       "  'vahoy',\n",
       "  'mirtprutasantapersonasservicio',\n",
       "  'mirtp',\n",
       "  'rutasvartprutasunidadesrutasi',\n",
       "  'rutasvartprutasunidadesrutasi',\n",
       "  '',\n",
       "  'unidadessivartp',\n",
       "  'rtpciudaddemex',\n",
       "  'unidadessi',\n",
       "  '',\n",
       "  'serviciohoyrtpciudadsiservicio',\n",
       "  'unidadesrtpciudad',\n",
       "  'hoy',\n",
       "  'rutartpciudaddemexdíaciudadrtpmetrociudad',\n",
       "  'mirtpservicio',\n",
       "  'mirtprutasanta',\n",
       "  'rtp',\n",
       "  'rutartphoyrtpciudaddemexsantapersonas'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96290903",
   "metadata": {},
   "source": [
    "### Creando la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9593f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13331ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los documentos en matrices con las `num_words` en modeo binario\n",
    "X_train = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "X_test = tokenizer.texts_to_matrix(test_docs, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2684ccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 31), (40, 31))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6a8891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener el número de características\n",
    "n_words = X_test.shape[1]  # Número de características (palabras) que se usarán\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca9d2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a float32 para compatibilidad\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "704b9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo de analisis de reseñas\n",
    "\n",
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_words,)))  # Usamos Input en lugar de input_shape\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f4085e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocad, tokenizer, model):\n",
    "    # Preprocesar el texto de la reseña\n",
    "    tokens = clean_text(review)\n",
    "    tokens = [w for w in tokens if w in vocab] # Filtrar palabras según el vocabulario\n",
    "    lines = ''.join(tokens)\n",
    "    \n",
    "    # Convertir la reseña a una matriz\n",
    "    encoded = tokenizer.texts_to_matrix([review], mode='binary')\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    y_pred = model.predict(encoded, verbose=0)\n",
    "    porcentaje_pos = y_pred[0][0]\n",
    "    \n",
    "    if round(porcentaje_pos) == 0:\n",
    "        return (1 - porcentaje_pos), 'Negativa'\n",
    "    return porcentaje_pos, 'Positiva'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "245d6d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">250,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │       \u001b[38;5;34m250,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,101</span> (976.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m250,101\u001b[0m (976.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,101</span> (976.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m250,101\u001b[0m (976.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 - 1s - 366ms/step - accuracy: 0.5250 - loss: 0.6938\n",
      "Epoch 2/10\n",
      "2/2 - 0s - 32ms/step - accuracy: 0.5250 - loss: 0.6901\n",
      "Epoch 3/10\n",
      "2/2 - 0s - 28ms/step - accuracy: 0.6000 - loss: 0.6871\n",
      "Epoch 4/10\n",
      "2/2 - 0s - 26ms/step - accuracy: 0.6000 - loss: 0.6842\n",
      "Epoch 5/10\n",
      "2/2 - 0s - 28ms/step - accuracy: 0.6750 - loss: 0.6812\n",
      "Epoch 6/10\n",
      "2/2 - 0s - 29ms/step - accuracy: 0.7500 - loss: 0.6785\n",
      "Epoch 7/10\n",
      "2/2 - 0s - 32ms/step - accuracy: 0.8250 - loss: 0.6758\n",
      "Epoch 8/10\n",
      "2/2 - 0s - 42ms/step - accuracy: 0.8250 - loss: 0.6730\n",
      "Epoch 9/10\n",
      "2/2 - 0s - 27ms/step - accuracy: 0.8500 - loss: 0.6703\n",
      "Epoch 10/10\n",
      "2/2 - 0s - 25ms/step - accuracy: 0.9000 - loss: 0.6674\n",
      "Accuracy test: 92.500001\n"
     ]
    }
   ],
   "source": [
    "# Crear el tokenizador y entrenarlo con los datos de entrenamiento\n",
    "tokenizer = Tokenizer(num_words=num_words)  # Limitar el tokenizador a las `num_words` más frecuentes\n",
    "tokenizer.fit_on_texts(train_docs)  # Entrenar el tokenizador con los documentos de entrenamiento\n",
    "\n",
    "# Convertir los documentos en matrices con las `num_words` más frecuentes\n",
    "X_train = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "X_test = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "\n",
    "# Obtener el número de características\n",
    "n_words = X_test.shape[1]  # Número de características (palabras) que se usarán\n",
    "\n",
    "# Definir el modelo\n",
    "model = define_model(n_words)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Imprimir la precisión en el conjunto de prueba\n",
    "print('Accuracy test: %f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e61d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review [No me gusta el servicio de atención al cliente. No me han resuelto el problema.]\n",
      "Sentiment: Negativa (52.800%):\n",
      "Review [Me gustaría que mejoraran el servicio de atención al cliente. Me gustaría que me llamen para resolver el problema.]\n",
      "Sentiment: Negativa (52.800%):\n"
     ]
    }
   ],
   "source": [
    "txt_queja = 'No me gusta el servicio de atención al cliente. No me han resuelto el problema.'\n",
    "\n",
    "perc_queja, review = predict_sentiment(txt_queja, vocab, tokenizer, model)\n",
    "print('Review [%s]\\nSentiment: %s (%.3f%%):' % (txt_queja, review, perc_queja * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6491204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me gustaría que mejoraran el servicio de atención al cliente. Me gustaría que me llamen para resolver el problema.\n",
      "Sentiment: Positiva (72.800%)\n"
     ]
    }
   ],
   "source": [
    "txt_sugerencia = 'Me gustaría que mejoraran el servicio de atención al cliente. Me gustaría que me llamen para resolver el problema.'\n",
    "\n",
    "perc_sugerencia, review2 = predict_sentiment(txt_sugerencia, vocab, tokenizer, model)\n",
    "print('Review [%s]\\nSentiment: %s (%.3f%%):' % (txt_sugerencia, review2, perc_sugerencia * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
